{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8卡爆杀LLaVA 1.6多图SFT LLaVA 1.6 Multiple images SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA Mistral Multiple Images SFT\n",
    "\n",
    "LLaVA是2023年4月提出的针对多模态场景的，可多轮图文问答ChatBot模型。LLaVA通过简单地把1024维输出的CLIP特征用projector和语言模型的embedding拼接起来，就能实现该效果。\n",
    "\n",
    "![image-20240331212648086](assets/images/image-20240331212648086.png)\n",
    "\n",
    "但是，在原文章中，作者是针对单图问答场景进行的训练，如果想实现一个**多图输入场景**的任务，应该如何改造结构以及构造训练数据呢？下面我们一起来看一下。\n",
    "\n",
    "## 代码结构\n",
    "\n",
    "### 启动命令\n",
    "\n",
    "```shell\n",
    "bash llava/scripts/v1_5/finetune.sh\n",
    "```\n",
    "\n",
    "### 训练入口\n",
    "\n",
    "```shell\n",
    "llava/train/train.py\n",
    "```\n",
    "\n",
    "### 训练框架\n",
    "\n",
    "1. 训练框架使用了Huggingface下的Trainer，Trainer是专门为了Transformer架构优化的训练器。进去之后可以看到作者了使用`deepspeed`训练框架，这里不再赘述。\n",
    "\n",
    "### Fine-tune的整体流程\n",
    "\n",
    "![llava_train.drawio](assets/images/llava_train.drawio.svg)\n",
    "\n",
    "\n",
    "\n",
    "### 关键代码\n",
    "\n",
    "#### 多轮对话预处理\n",
    "\n",
    "图像标记将在分词后的提示文本块之间插入。以下是该功能工作原理的分解：\n",
    "\n",
    "1. 提示通过 `<image>` 标记分割，创建一个块的列表。\n",
    "2. 使用提供的分词器对每个块进行分词，得到一个令牌 ID 的列表。\n",
    "3. 使用 `insert_separator` 函数将分词后的块列表与 `image_token_index`（代表图像的令牌）交错插入。\n",
    "4. `input_ids` 列表如下构建：\n",
    "   - 如果第一个分词后的块以序列开始（BOS）令牌开头，则 `input_ids` 的第一个元素设置为该 BOS 令牌。\n",
    "   - 通过迭代交错的分词块列表和 `image_token_index` 填充 `input_ids` 的剩余元素。\n",
    "\n",
    "因此，结果的 `input_ids` 列表将具有以下结构：\n",
    "\n",
    "```\n",
    "[BOS_token（如果存在），tokens_from_chunk1, image_token, tokens_from_chunk2, image_token, ..., tokens_from_last_chunk]\n",
    "```\n",
    "\n",
    "`image_token_index` 将插入原始提示中每对连续块之间。\n",
    "\n",
    "例如，如果提示是 `\"This is <image> a sample <image> prompt\"`，且 `image_token_index` 是 1234，结果的 `input_ids` 列表可能看起来像：\n",
    "\n",
    "```\n",
    "[101, 1010, 2003, 1015, 1234, 2034, 3076, 1234, 2001, 1028, 102]\n",
    "```\n",
    "\n",
    "这里，令牌 ID 代表分词的单词，而值 1234 是插入块之间的 `image_token_index`。\n",
    "\n",
    "\n",
    "\n",
    "## 大致改动\n",
    "\n",
    "要适应多图训练，首先要判断自己的任务是要图片和文字interleaved的形式还是separate的形式。\n",
    "\n",
    "1. 数据预处理：确保Input conversation中的image_token被正确替换了；\n",
    "2. Model Forward：确保训练input_embedding是否按照期望顺序被cat在一起了。\n",
    "\n",
    "注意，因为LLaVA本身SFT时候，是把所有image的embedding都放到了最前面（通过对话预处理实现的），因此如果你训练改成interleaved的形式，可能导致其本身SFT Align的分布变化。\n",
    "\n",
    "\n",
    "\n",
    "## 预训练数据组织\n",
    "\n",
    "原SFT训练数据格式，为了展示用，复制了两条数据\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Please tell me what's unusual about this image: <image>\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"A man is ironing his clothes on a vehicle. \"},\n",
    "            {\"from\": \"human\", \"value\": \"What's funny about this?\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Because people don't usually do this at home.\"}\n",
    "        ],\n",
    "        \"image\": \"llava/image_folder(local image path)\"\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Please tell me what's unusual about this image: <image>\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"A man is ironing his clothes on a vehicle. \"},\n",
    "            {\"from\": \"human\", \"value\": \"What's funny about this?\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Because people don't usually do this at home.\"}\n",
    "        ],\n",
    "        \"image\": \"llava/image_folder(local image path)\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "改动后SFT训练数据格式：\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Please tell me what's unusual about this image: <image>\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"A man is ironing his clothes on a vehicle. \"},\n",
    "            {\"from\": \"human\", \"value\": \"What's funny about this?\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Because people don't usually do this at home.\"}\n",
    "        ],\n",
    "        \"images\": [\"llava/image_folder(local image path)\", \"llava/image_folder(local image path)\"]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Please tell me what's unusual about this image: <image>\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"A man is ironing his clothes on a vehicle. \"},\n",
    "            {\"from\": \"human\", \"value\": \"What's funny about this?\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Because people don't usually do this at home.\"}\n",
    "        ],\n",
    "        \"images\": [\"llava/image_folder(local image path)\", \"llava/image_folder(local image path)\"]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 代码改动\n",
    "\n",
    "1. （optional）修改image token的位置，我们把stack在前面的1个换成多个\n",
    "\n",
    "```python\n",
    "# llava/train/train.py\n",
    "def preprocess_multimodal(\n",
    "    sources: Sequence[str],\n",
    "    data_args: DataArguments\n",
    ") -> Dict:\n",
    "    is_multimodal = data_args.is_multimodal\n",
    "    if not is_multimodal:\n",
    "        return sources\n",
    "\n",
    "    for source in sources:\n",
    "        for sentence in source:\n",
    "            if DEFAULT_IMAGE_TOKEN in sentence['value']:\n",
    "                replace_token = DEFAULT_IMAGE_TOKEN + '\\n'\n",
    "                sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, replace_token).strip()\n",
    "                # sentence['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + sentence['value']\n",
    "                sentence['value'] = sentence['value'].strip()\n",
    "                if \"mmtag\" in conversation_lib.default_conversation.version:\n",
    "                    sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '<Image>' + DEFAULT_IMAGE_TOKEN + '</Image>')\n",
    "            replace_token = DEFAULT_IMAGE_TOKEN\n",
    "            if data_args.mm_use_im_start_end:\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "\n",
    "    return sources\n",
    "```\n",
    "\n",
    "2. 修改多图Input\n",
    "\n",
    "```python\n",
    "# llava/train/train.py LazySupervisedDataset\n",
    "def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "        if 'image' in sources[0]:\n",
    "            image_files = self.list_data_dict[i]['images']\n",
    "            image_folder = self.data_args.image_folder\n",
    "            processor = self.data_args.image_processor\n",
    "            images = []\n",
    "            for image in image_files:\n",
    "                image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\n",
    "                if self.data_args.image_aspect_ratio == 'pad':\n",
    "                    def expand2square(pil_img, background_color):\n",
    "                        width, height = pil_img.size\n",
    "                        if width == height:\n",
    "                            return pil_img\n",
    "                        elif width > height:\n",
    "                            result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                            result.paste(pil_img, (0, (width - height) // 2))\n",
    "                            return result\n",
    "                        else:\n",
    "                            result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                            result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                            return result\n",
    "                    image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n",
    "                    image = processor.preprocess(image, return_tensors='pt')['pixel_values']\n",
    "                else:\n",
    "                    image = processor.preprocess(image, return_tensors='pt')['pixel_values']\n",
    "                images.append(image)\n",
    "                sources = preprocess_multimodal(\n",
    "                    copy.deepcopy([e[\"conversations\"] for e in sources]),\n",
    "                    self.data_args)\n",
    "        else:\n",
    "            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\n",
    "        data_dict = preprocess(\n",
    "            sources,\n",
    "            self.tokenizer,\n",
    "            has_image=('image' in self.list_data_dict[i]))\n",
    "        if isinstance(i, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                             labels=data_dict[\"labels\"][0])\n",
    "\n",
    "        # image exist in the data\n",
    "        if 'images' in self.list_data_dict[i]:\n",
    "            data_dict['images'] = images\n",
    "        elif self.data_args.is_multimodal:\n",
    "            # image does not exist in the data, but the model is multimodal\n",
    "            crop_size = self.data_args.image_processor.crop_size\n",
    "            data_dict['images'] = [torch.zeros(3, crop_size['height'], crop_size['width'])]\n",
    "        return data_dict\n",
    "```\n",
    "\n",
    "3. 修改batch Input\n",
    "\n",
    "```python\n",
    "# llava/train/train.py\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "        if 'image' in instances[0]:\n",
    "            images = [instance['images'] for instance in instances]\n",
    "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                batch['images'] = torch.stack(images)\n",
    "            else:\n",
    "                batch['images'] = images\n",
    "\n",
    "        return batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多图inference脚本\n",
    "\n",
    "```shell\n",
    "cd llava\n",
    "python3 scripts/llava_1_6_generate.py --pretrained_path /mnt/bn/chengru-nas/models/llava-v1.6-mistral-7b --max_length 4096\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "The image you've provided is a composite of three different images, each depicting a scene with a character that appears to be a red, anthropomorphic creature with flames, likely representing a dragon or a mythical creature. The creature has large, black eyes and is wearing glasses.\n",
    "\n",
    "1. On the top left, there's a title \"LLaMA (2)\" followed by a description that reads: \"What is unusual about this image? This is a 5 year old's interpretation of a llama.\" Below this description, there's a scale with various parts of the creature labeled with percentages, such as \"VQa2\" with 80.0, \"MMVet\" with 80.0, \"GQA\" with 80.0, \"VizWiz\" with 80.0, \"LLaMA\" with 80.0, \"VQa\" with 80.0, \"MMVet\" with 80.0, and \"GQA\" with 80.0.\n",
    "\n",
    "2. On the top right, the title \"LLaMA (2)\" followed by a description that reads: \"What is unusual about this image? This is a 5 year old's interpretation of a llama.\" Below this description, there's a scale with various parts of the creature labeled with percentages, such as \"VQa2\" with 53.6, \"MMVet\" with 53.6, \"GQA\" with 53.6, \"VizWiz\" with 53.6, \"LLaMA\" with 53.6, \"VQa\" with 53.6, \"MMVet\" with 53.6, and \"GQA\" with 53.6.\n",
    "\n",
    "3. On the bottom, there's a title \"LLaMA (2)\" followed by a description that reads: \"What is unusual about this image? This is a 5 year old's interpretation of a llama.\" Below this description, there's a scale with various parts of the creature labeled with percentages, such as \"VQa2\" with 159.8, \"MMVet\" with 159.8, \"GQA\" with 159.8, \"VizWiz\" with 159.8, \"LLaMA\" with 159.8, \"VQa\" with 159.8, \"MMVet\" with 159.8, and \"GQA\" with 159.8.\n",
    "\n",
    "Each image seems to be part of an analysis or comparison, possibly related to artificial intelligence or computer vision tasks, where the accuracy of a model in identifying objects or scenes is being evaluated. The percentages likely represent the confidence score or accuracy rate of the model in identifying the creature as a llama. The images are likely used to test the performance of a system, with the \"LLaMA (2)\" title possibly indicating a specific version or configuration of the model.\n",
    "```\n",
    "\n",
    "## 启动脚本\n",
    "\n",
    "```shell\n",
    "bash scripts/v1_5/finetune.sh > test.log 2>&1\n",
    "```\n",
    "\n",
    "## 主要改动\n",
    "\n",
    "1. conversation预处理支持多图\n",
    "\n",
    "   1. ![image-20240418151654440](/assets/images/image-20240418151654440.png)\n",
    "      ```python\n",
    "      def get_prompt(self):\n",
    "              messages = self.messages\n",
    "              if len(messages) > 0 and type(messages[0][1]) is tuple:\n",
    "                  messages = self.messages.copy()\n",
    "                  init_role, init_msg = messages[0].copy()\n",
    "                  img_cnt = init_msg[0].count(\"<image>\")\n",
    "                  init_msg = init_msg[0].replace(\"<image>\", \"\").strip()\n",
    "                  if 'mmtag' in self.version:\n",
    "                      messages[0] = (init_role, init_msg)\n",
    "                      messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n",
    "                      messages.insert(1, (self.roles[1], \"Received.\"))\n",
    "                  else:\n",
    "                      messages[0] = (init_role, \"<image>\\n\"*img_cnt + init_msg)\n",
    "      ```\n",
    "\n",
    "      \n",
    "\n",
    "2. 修改DataLoader适配多图输入\n",
    "\n",
    "   1. ![image-20240418151356393](/assets/images/image-20240418151356393.png)\n",
    "      ```python\n",
    "      # llava/train/train.py#L721\n",
    "      if 'images' in sources[0]:\n",
    "                  image_b64 = self.list_data_dict[i]['images']\n",
    "                  # image_folder = self.data_args.image_folder\n",
    "                  images = [load_image_from_base64(image) for image in image_b64]\n",
    "                  images = process_images(images, self.data_args.image_processor, self.data_args.model_cfg)\n",
    "      ```\n",
    "\n",
    "3. 修改batch DataLoader支持batch图片输入\n",
    "\n",
    "   1. ![image-20240418151440594](/assets/images/image-20240418151440594.png)\n",
    "      ```python\n",
    "      # llava/train/train.py#L800\n",
    "      if 'images' in instances[0]:\n",
    "                  images = [instance['images'] for instance in instances]\n",
    "                  if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                      batch['images'] = torch.stack(images)\n",
    "                  else:\n",
    "                      batch['images'] = images\n",
    "                  batch[\"images\"] = torch.stack([image_tensor.type(torch.bfloat16) for image_tensor in images], dim=0)\n",
    "      ```\n",
    "\n",
    "4. 支持batch Encode image\n",
    "\n",
    "   1. ![image-20240418151537355](/assets/images/image-20240418151537355.png)\n",
    "      ```python\n",
    "      # llava/model/llava_arch.py\n",
    "      if (isinstance(images, list) and images[0].ndim == 3) or images.ndim == 4:\n",
    "                  image_features = self.get_model().get_vision_tower()(images)\n",
    "                  image_features = [self.get_model().mm_projector(image_feature) for image_feature in image_features]\n",
    "              else:\n",
    "                  image_features = [self.get_model().get_vision_tower()(image) for image in images]\n",
    "                  res_features = []\n",
    "                  for image_feature in image_features:\n",
    "                      temp_features = []\n",
    "                      for temp_feature in image_feature:\n",
    "                          temp_features.append(self.get_model().mm_projector(temp_feature))\n",
    "                      temp_features = torch.stack(temp_features, dim=0)\n",
    "                      res_features.append(temp_features)\n",
    "                  image_features = torch.stack(res_features, dim=0)\n",
    "      ```\n",
    "\n",
    "5. 开始训练\n",
    "\n",
    "   1. ![image-20240418150359558](/assets/images/image-20240418150359558.png)\n",
    "\n",
    "   2. ```log\n",
    "       0%|          | 1/1110 [00:08<2:41:27,  8.74s/it]\n",
    "                                                        \n",
    "      {'loss': 2.3617, 'grad_norm': 58.519387034620586, 'learning_rate': 5.882352941176471e-07, 'epoch': 0.0}\n",
    "      \n",
    "        0%|          | 1/1110 [00:08<2:41:27,  8.74s/it]\n",
    "        0%|          | 2/1110 [00:11<1:37:16,  5.27s/it]\n",
    "                                                        \n",
    "      {'loss': 1.87, 'grad_norm': 63.3572221448974, 'learning_rate': 1.1764705882352942e-06, 'epoch': 0.0}\n",
    "      \n",
    "        0%|          | 2/1110 [00:11<1:37:16,  5.27s/it]\n",
    "        0%|          | 3/1110 [00:13<1:12:48,  3.95s/it]\n",
    "                                                        \n",
    "      {'loss': 1.8126, 'grad_norm': 52.72796065772046, 'learning_rate': 1.7647058823529414e-06, 'epoch': 0.0}\n",
    "      \n",
    "        0%|          | 3/1110 [00:13<1:12:48,  3.95s/it]\n",
    "        0%|          | 4/1110 [00:16<1:01:45,  3.35s/it]\n",
    "                                                        \n",
    "      {'loss': 1.7951, 'grad_norm': 40.40844027020016, 'learning_rate': 2.3529411764705885e-06, 'epoch': 0.0}\n",
    "      \n",
    "        0%|          | 4/1110 [00:16<1:01:45,  3.35s/it]\n",
    "        0%|          | 5/1110 [00:18<55:09,  3.00s/it]  \n",
    "                                                      \n",
    "      {'loss': 1.6276, 'grad_norm': 34.06122108635625, 'learning_rate': 2.9411764705882355e-06, 'epoch': 0.0}\n",
    "      \n",
    "        0%|          | 5/1110 [00:18<55:09,  3.00s/it]\n",
    "        1%|          | 6/1110 [00:21<51:07,  2.78s/it]\n",
    "                                                      \n",
    "      {'loss': 1.4333, 'grad_norm': 35.78034912961043, 'learning_rate': 3.529411764705883e-06, 'epoch': 0.01}\n",
    "      \n",
    "        1%|          | 6/1110 [00:21<51:07,  2.78s/it]\n",
    "        1%|          | 7/1110 [00:23<48:35,  2.64s/it]\n",
    "      ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
