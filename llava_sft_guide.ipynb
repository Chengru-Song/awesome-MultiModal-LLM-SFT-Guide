{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8卡爆杀LLaVA 1.6多图SFT LLaVA 1.6 Multiple images SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA Mistral Multiple Images SFT\n",
    "\n",
    "LLaVA是2023年4月提出的针对多模态场景的，可多轮图文问答ChatBot模型。LLaVA通过简单地把1024维输出的CLIP特征用projector和语言模型的embedding拼接起来，就能实现该效果。\n",
    "\n",
    "![image-20240331212648086](assets/images/image-20240331212648086.png)\n",
    "\n",
    "但是，在原文章中，作者是针对单图问答场景进行的训练，如果想实现一个**多图输入场景**的任务，应该如何改造结构以及构造训练数据呢？下面我们一起来看一下。\n",
    "\n",
    "## 代码结构\n",
    "\n",
    "### 启动命令\n",
    "\n",
    "```shell\n",
    "bash llava/scripts/v1_5/finetune.sh\n",
    "```\n",
    "\n",
    "### 训练入口\n",
    "\n",
    "```shell\n",
    "llava/train/train.py\n",
    "```\n",
    "\n",
    "### 训练框架\n",
    "\n",
    "1. 训练框架使用了Huggingface下的Trainer，Trainer是专门为了Transformer架构优化的训练器。进去之后可以看到作者了使用`deepspeed`训练框架，这里不再赘述。\n",
    "\n",
    "### Fine-tune的整体流程\n",
    "\n",
    "![llava_train.drawio](assets/images/llava_train.drawio.svg)\n",
    "\n",
    "\n",
    "\n",
    "### 关键代码\n",
    "\n",
    "#### 多轮对话预处理\n",
    "\n",
    "图像标记将在分词后的提示文本块之间插入。以下是该功能工作原理的分解：\n",
    "\n",
    "1. 提示通过 `<image>` 标记分割，创建一个块的列表。\n",
    "2. 使用提供的分词器对每个块进行分词，得到一个令牌 ID 的列表。\n",
    "3. 使用 `insert_separator` 函数将分词后的块列表与 `image_token_index`（代表图像的令牌）交错插入。\n",
    "4. `input_ids` 列表如下构建：\n",
    "   - 如果第一个分词后的块以序列开始（BOS）令牌开头，则 `input_ids` 的第一个元素设置为该 BOS 令牌。\n",
    "   - 通过迭代交错的分词块列表和 `image_token_index` 填充 `input_ids` 的剩余元素。\n",
    "\n",
    "因此，结果的 `input_ids` 列表将具有以下结构：\n",
    "\n",
    "```\n",
    "[BOS_token（如果存在），tokens_from_chunk1, image_token, tokens_from_chunk2, image_token, ..., tokens_from_last_chunk]\n",
    "```\n",
    "\n",
    "`image_token_index` 将插入原始提示中每对连续块之间。\n",
    "\n",
    "例如，如果提示是 `\"This is <image> a sample <image> prompt\"`，且 `image_token_index` 是 1234，结果的 `input_ids` 列表可能看起来像：\n",
    "\n",
    "```\n",
    "[101, 1010, 2003, 1015, 1234, 2034, 3076, 1234, 2001, 1028, 102]\n",
    "```\n",
    "\n",
    "这里，令牌 ID 代表分词的单词，而值 1234 是插入块之间的 `image_token_index`。\n",
    "\n",
    "\n",
    "\n",
    "## 大致改动\n",
    "\n",
    "要适应多图训练，首先要判断自己的任务是要图片和文字interleaved的形式还是separate的形式。\n",
    "\n",
    "1. 数据预处理：确保Input conversation中的image_token被正确替换了；\n",
    "2. Model Forward：确保训练input_embedding是否按照期望顺序被cat在一起了。\n",
    "\n",
    "注意，因为LLaVA本身SFT时候，是把所有image的embedding都放到了最前面（通过对话预处理实现的），因此如果你训练改成interleaved的形式，可能导致其本身SFT Align的分布变化。\n",
    "\n",
    "\n",
    "\n",
    "## 预训练数据组织\n",
    "\n",
    "原SFT训练数据格式，为了展示用，复制了两条数据\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Please tell me what's unusual about this image: <image>\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"A man is ironing his clothes on a vehicle. \"},\n",
    "            {\"from\": \"human\", \"value\": \"What's funny about this?\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Because people don't usually do this at home.\"}\n",
    "        ],\n",
    "        \"image\": \"llava/image_folder(local image path)\"\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Please tell me what's unusual about this image: <image>\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"A man is ironing his clothes on a vehicle. \"},\n",
    "            {\"from\": \"human\", \"value\": \"What's funny about this?\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Because people don't usually do this at home.\"}\n",
    "        ],\n",
    "        \"image\": \"llava/image_folder(local image path)\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "改动后SFT训练数据格式：\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Please tell me what's unusual about this image: <image>\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"A man is ironing his clothes on a vehicle. \"},\n",
    "            {\"from\": \"human\", \"value\": \"What's funny about this?\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Because people don't usually do this at home.\"}\n",
    "        ],\n",
    "        \"images\": [\"llava/image_folder(local image path)\", \"llava/image_folder(local image path)\"]\n",
    "    },\n",
    "    {\n",
    "        \"conversations\": [\n",
    "            {\"from\": \"human\", \"value\": \"Please tell me what's unusual about this image: <image>\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"A man is ironing his clothes on a vehicle. \"},\n",
    "            {\"from\": \"human\", \"value\": \"What's funny about this?\"},\n",
    "            {\"from\": \"gpt\", \"value\": \"Because people don't usually do this at home.\"}\n",
    "        ],\n",
    "        \"images\": [\"llava/image_folder(local image path)\", \"llava/image_folder(local image path)\"]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 代码改动\n",
    "\n",
    "1. （optional）修改image token的位置，我们把stack在前面的1个换成多个\n",
    "\n",
    "```python\n",
    "# llava/train/train.py\n",
    "def preprocess_multimodal(\n",
    "    sources: Sequence[str],\n",
    "    data_args: DataArguments\n",
    ") -> Dict:\n",
    "    is_multimodal = data_args.is_multimodal\n",
    "    if not is_multimodal:\n",
    "        return sources\n",
    "\n",
    "    for source in sources:\n",
    "        for sentence in source:\n",
    "            if DEFAULT_IMAGE_TOKEN in sentence['value']:\n",
    "                replace_token = DEFAULT_IMAGE_TOKEN + '\\n'\n",
    "                sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, replace_token).strip()\n",
    "                # sentence['value'] = DEFAULT_IMAGE_TOKEN + '\\n' + sentence['value']\n",
    "                sentence['value'] = sentence['value'].strip()\n",
    "                if \"mmtag\" in conversation_lib.default_conversation.version:\n",
    "                    sentence['value'] = sentence['value'].replace(DEFAULT_IMAGE_TOKEN, '<Image>' + DEFAULT_IMAGE_TOKEN + '</Image>')\n",
    "            replace_token = DEFAULT_IMAGE_TOKEN\n",
    "            if data_args.mm_use_im_start_end:\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "            sentence[\"value\"] = sentence[\"value\"].replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "\n",
    "    return sources\n",
    "```\n",
    "\n",
    "2. 修改多图Input\n",
    "\n",
    "```python\n",
    "# llava/train/train.py LazySupervisedDataset\n",
    "def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "        if 'image' in sources[0]:\n",
    "            image_files = self.list_data_dict[i]['images']\n",
    "            image_folder = self.data_args.image_folder\n",
    "            processor = self.data_args.image_processor\n",
    "            images = []\n",
    "            for image in image_files:\n",
    "                image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\n",
    "                if self.data_args.image_aspect_ratio == 'pad':\n",
    "                    def expand2square(pil_img, background_color):\n",
    "                        width, height = pil_img.size\n",
    "                        if width == height:\n",
    "                            return pil_img\n",
    "                        elif width > height:\n",
    "                            result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                            result.paste(pil_img, (0, (width - height) // 2))\n",
    "                            return result\n",
    "                        else:\n",
    "                            result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                            result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                            return result\n",
    "                    image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n",
    "                    image = processor.preprocess(image, return_tensors='pt')['pixel_values']\n",
    "                else:\n",
    "                    image = processor.preprocess(image, return_tensors='pt')['pixel_values']\n",
    "                images.append(image)\n",
    "                sources = preprocess_multimodal(\n",
    "                    copy.deepcopy([e[\"conversations\"] for e in sources]),\n",
    "                    self.data_args)\n",
    "        else:\n",
    "            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\n",
    "        data_dict = preprocess(\n",
    "            sources,\n",
    "            self.tokenizer,\n",
    "            has_image=('image' in self.list_data_dict[i]))\n",
    "        if isinstance(i, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                             labels=data_dict[\"labels\"][0])\n",
    "\n",
    "        # image exist in the data\n",
    "        if 'images' in self.list_data_dict[i]:\n",
    "            data_dict['images'] = images\n",
    "        elif self.data_args.is_multimodal:\n",
    "            # image does not exist in the data, but the model is multimodal\n",
    "            crop_size = self.data_args.image_processor.crop_size\n",
    "            data_dict['images'] = [torch.zeros(3, crop_size['height'], crop_size['width'])]\n",
    "        return data_dict\n",
    "```\n",
    "\n",
    "3. 修改batch Input\n",
    "\n",
    "```python\n",
    "# llava/train/train.py\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "        if 'image' in instances[0]:\n",
    "            images = [instance['images'] for instance in instances]\n",
    "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                batch['images'] = torch.stack(images)\n",
    "            else:\n",
    "                batch['images'] = images\n",
    "\n",
    "        return batch\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
